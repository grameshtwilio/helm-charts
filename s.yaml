---
# Source: spire/charts/spiffe-csi-driver/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-spiffe-csi-driver
  namespace: default
  labels:
    helm.sh/chart: spiffe-csi-driver-0.1.0
    app.kubernetes.io/name: spiffe-csi-driver
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-agent/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-agent
  namespace: default
  labels:
    helm.sh/chart: spire-agent-0.1.0
    app.kubernetes.io/name: agent
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-server/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-server
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
---
# Source: spire/charts/spire-agent/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-agent
  namespace: default
data:
  agent.conf: |
    {
      "agent": {
        "data_dir": "/run/spire",
        "log_level": "info",
        "sds": {
          "default_all_bundles_name": "ALL",
          "default_bundle_name": "ROOTCA",
          "default_svid_name": "default",
          "disable_spiffe_cert_validation": false
        },
        "server_address": "release-name-server.default",
        "server_port": "8081",
        "socket_path": "/run/spire/agent-sockets/spire-agent.sock",
        "trust_bundle_path": "/run/spire/bundle/bundle.crt",
        "trust_domain": "example.org"
      },
      "health_checks": {
        "bind_address": "0.0.0.0",
        "bind_port": "9980",
        "listener_enabled": true,
        "live_path": "/live",
        "ready_path": "/ready"
      },
      "plugins": {
        "KeyManager": [
          {
            "memory": {
              "plugin_data": null
            }
          }
        ],
        "NodeAttestor": [
          {
            "k8s_psat": {
              "plugin_data": {
                "cluster": "example-cluster"
              }
            }
          }
        ],
        "WorkloadAttestor": [
          {
            "k8s": {
              "plugin_data": {
                "disable_container_selectors": false,
                "skip_kubelet_verification": true
              }
            }
          }
        ]
      }
    }
---
# Source: spire/charts/spire-server/templates/bundle-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spire-bundle
  namespace: default
---
# Source: spire/charts/spire-server/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-server
  namespace: default
data:
  server.conf: |
    {
      "health_checks": {
        "bind_address": "0.0.0.0",
        "bind_port": "8080",
        "listener_enabled": true,
        "live_path": "/live",
        "ready_path": "/ready"
      },
      "plugins": {
        "DataStore": [
          {
            "sql": {
              "plugin_data": {
                "connection_string": "/run/spire/data/datastore.sqlite3",
                "database_type": "sqlite3"
              }
            }
          }
        ],
        "KeyManager": [
          {
            "disk": {
              "plugin_data": {
                "keys_path": "/run/spire/data/keys.json"
              }
            }
          }
        ],
        "NodeAttestor": [
          {
            "k8s_psat": {
              "plugin_data": {
                "clusters": {
                  "example-cluster": {
                    "service_account_allow_list": [
                      "default:release-name-agent"
                    ]
                  }
                }
              }
            }
          }
        ],
        "Notifier": [
          {
            "k8sbundle": {
              "plugin_data": {
                "config_map": "spire-bundle",
                "namespace": "default"
              }
            }
          }
        ]
      },
      "server": {
        "bind_address": "0.0.0.0",
        "bind_port": "8081",
        "ca_key_type": "rsa-2048",
        "ca_subject": [
          {
            "common_name": "example.org",
            "country": [
              "NL"
            ],
            "organization": [
              "Example"
            ]
          }
        ],
        "ca_ttl": "24h",
        "data_dir": "/run/spire/data",
        "default_jwt_svid_ttl": "1h",
        "default_x509_svid_ttl": "4h",
        "jwt_issuer": "oidc-discovery.example.org",
        "log_level": "info",
        "trust_domain": "example.org"
      }
    }
---
# Source: spire/charts/spire-server/templates/controller-manager-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-controller-manager
  namespace: default
data:
  controller-manager-config.yaml: |
    apiVersion: spire.spiffe.io/v1alpha1
    kind: ControllerManagerConfig
    metadata:
      name: release-name-controller-manager
      namespace: default
      labels:
        helm.sh/chart: spire-server-0.1.0
        app.kubernetes.io/name: server
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "1.7.2"
        app.kubernetes.io/managed-by: Helm
    metrics:
      bindAddress: 0.0.0.0:8082
    health:
      healthProbeBindAddress: 0.0.0.0:8083
    leaderElection:
      leaderElect: true
      resourceName: 90b19a18.spiffe.io
      resourceNamespace: default
    validatingWebhookConfigurationName: release-name-controller-manager-webhook
    clusterName: example-cluster
    trustDomain: example.org
    ignoreNamespaces:
      - kube-system
      - kube-public
      - local-path-storage
    spireServerSocketPath: "/tmp/spire-server/private/api.sock"
---
# Source: spire/charts/spire-agent/templates/roles.yaml
# Required cluster role to allow spire-agent to query k8s API server
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-agent
rules:
  - apiGroups: [""]
    resources:
      - pods
      - nodes
      - nodes/proxy
    verbs: ["get"]
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-controller-manager
rules:
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    verbs: ["get", "list", "patch", "watch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains/finalizers"]
    verbs: ["update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterfederatedtrustdomains/status"]
    verbs: ["get", "patch", "update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids/finalizers"]
    verbs: ["update"]
  - apiGroups: ["spire.spiffe.io"]
    resources: ["clusterspiffeids/status"]
    verbs: ["get", "patch", "update"]
---
# Source: spire/charts/spire-server/templates/roles.yaml
# ClusterRole to allow spire-server node attestor to query Token Review API
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: default-release-name-server
rules:
  - apiGroups: [authentication.k8s.io]
    resources: [tokenreviews]
    verbs:
      - get
      - watch
      - list
      - create
  - apiGroups: [""]
    resources: [nodes, pods]
    verbs:
      - get
      - list
---
# Source: spire/charts/spire-agent/templates/roles.yaml
# Binds above cluster role to spire-agent service account
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-agent
subjects:
  - kind: ServiceAccount
    name: release-name-agent
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-agent
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-controller-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-controller-manager
subjects:
- kind: ServiceAccount
  name: release-name-server
  namespace: default
---
# Source: spire/charts/spire-server/templates/roles.yaml
# Binds above cluster role to spire-server service account
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: default-release-name-server
subjects:
  - kind: ServiceAccount
    name: release-name-server
    namespace: default
roleRef:
  kind: ClusterRole
  name: default-release-name-server
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-controller-manager-leader-election
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
---
# Source: spire/charts/spire-server/templates/roles.yaml
# Role to be able to push certificate bundles to a configmap
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-bundle
  namespace: default
rules:
  - apiGroups: [""]
    resources: [configmaps]
    resourceNames: [spire-bundle]
    verbs:
      - get
      - patch
---
# Source: spire/charts/spire-server/templates/controller-manager-roles.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-controller-manager-leader-election
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-controller-manager-leader-election
subjects:
- kind: ServiceAccount
  name: release-name-server
  namespace: default
---
# Source: spire/charts/spire-server/templates/roles.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: spire-bundle
  namespace: default
subjects:
  - kind: ServiceAccount
    name: release-name-server
    namespace: default
roleRef:
  kind: Role
  name: spire-bundle
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/controller-manager-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-controller-manager-webhook
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: https
      port: 443
      targetPort: https
      protocol: TCP
  selector:
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
---
# Source: spire/charts/spire-server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-server
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: grpc
      port: 8081
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
---
# Source: spire/charts/spiffe-csi-driver/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-spiffe-csi-driver
  namespace: default
  labels:
    helm.sh/chart: spiffe-csi-driver-0.1.0
    app.kubernetes.io/name: spiffe-csi-driver
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: spiffe-csi-driver
      app.kubernetes.io/instance: release-name
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: spiffe-csi-driver
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-spiffe-csi-driver
      containers:
        # This is the container which runs the SPIFFE CSI driver.
        - name: spiffe-csi-driver
          image: ghcr.io/spiffe/spiffe-csi-driver:0.2.3
          imagePullPolicy: IfNotPresent
          args: [
            "-workload-api-socket-dir", "/spire-agent-socket",
            "-plugin-name", "csi.spiffe.io",
            "-csi-socket-path", "/spiffe-csi/csi.sock",
          ]
          env:
            # The CSI driver needs a unique node ID. The node name can be
            # used for this purpose.
            - name: MY_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            # The volume containing the SPIRE agent socket. The SPIFFE CSI
            # driver will mount this directory into containers.
            - mountPath: /spire-agent-socket
              name: spire-agent-socket-dir
              readOnly: true
            # The volume that will contain the CSI driver socket shared
            # with the kubelet and the driver registrar.
            - mountPath: /spiffe-csi
              name: spiffe-csi-socket-dir
            # The volume containing mount points for containers.
            - mountPath: /var/lib/kubelet/pods
              mountPropagation: Bidirectional
              name: mountpoint-dir
          securityContext:
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - all
            privileged: true
          resources:
            {}
        # This container runs the CSI Node Driver Registrar which takes care
        # of all the little details required to register a CSI driver with
        # the kubelet.
        - name: node-driver-registrar
          image: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0
          imagePullPolicy: IfNotPresent
          args: [
            "-csi-address", "/spiffe-csi/csi.sock",
            "-kubelet-registration-path", "/var/lib/kubelet/plugins/csi.spiffe.io/csi.sock",
            "-health-port", "9809"
          ]
          volumeMounts:
            # The registrar needs access to the SPIFFE CSI driver socket
            - mountPath: /spiffe-csi
              name: spiffe-csi-socket-dir
            # The registrar needs access to the Kubelet plugin registration
            # directory
            - name: kubelet-plugin-registration-dir
              mountPath: /registration
          ports:
            - containerPort: 9809
              name: healthz
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
            initialDelaySeconds: 5
            timeoutSeconds: 5
          resources:
            {}
      volumes:
        - name: spire-agent-socket-dir
          hostPath:
            path: /run/spire/agent-sockets
            type: DirectoryOrCreate
        # This volume is where the socket for kubelet->driver communication lives
        - name: spiffe-csi-socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/csi.spiffe.io
            type: DirectoryOrCreate
        # This volume is where the SPIFFE CSI driver mounts volumes
        - name: mountpoint-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
        # This volume is where the node-driver-registrar registers the plugin
        # with kubelet
        - name: kubelet-plugin-registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry
            type: Directory
---
# Source: spire/charts/spire-agent/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-agent
  namespace: default
  labels:
    helm.sh/chart: spire-agent-0.1.0
    app.kubernetes.io/name: agent
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: agent
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        checksum/config: 4474805d436c88b8706c02e70e1575cf3d6adde92385a62a847e748c01172a63
      labels:
        app.kubernetes.io/name: agent
        app.kubernetes.io/instance: release-name
    spec:
      hostPID: true
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: release-name-agent
      securityContext:
        {}
      initContainers:
        - name: init
          # This is a small image with wait-for-it, choose whatever image
          # you prefer that waits for a service to be up. This image is built
          # from https://github.com/vishnubob/wait-for-it
          image: cgr.dev/chainguard/wait-for-it:latest@sha256:deeaccb164a67a4d7f585c4d416641b1f422c029911a29d72beae28221f823df
          imagePullPolicy: IfNotPresent
          args: ["-t", "30", "-h", "release-name-server.default", "-p", "8081"]
          resources:
            {}
      containers:
        - name: spire-agent
          image: ghcr.io/spiffe/spire-agent:1.7.2
          imagePullPolicy: IfNotPresent
          args: ["-config", "/run/spire/config/agent.conf"]
          ports:
            - containerPort: 9980
              name: healthz
          volumeMounts:
            - name: spire-config
              mountPath: /run/spire/config
              readOnly: true
            - name: spire-bundle
              mountPath: /run/spire/bundle
              readOnly: true
            - name: spire-agent-socket-dir
              mountPath: /run/spire/agent-sockets
              readOnly: false
            - name: spire-token
              mountPath: /var/run/secrets/tokens
          livenessProbe:
            httpGet:
              path: /live
              port: healthz
            initialDelaySeconds: 15
            periodSeconds: 60
          readinessProbe:
            httpGet:
              path: /ready
              port: healthz
            initialDelaySeconds: 15
            periodSeconds: 60
          resources:
            {}
      volumes:
        - name: spire-config
          configMap:
            name: release-name-agent
        - name: spire-bundle
          configMap:
            name: spire-bundle
        - name: spire-token
          projected:
            sources:
            - serviceAccountToken:
                path: spire-agent
                expirationSeconds: 7200
                audience: spire-server
        - name: spire-agent-socket-dir
          hostPath:
            path: /run/spire/agent-sockets
            type: DirectoryOrCreate
---
# Source: spire/charts/spire-server/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-server
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: server
spec:
  replicas: 1
  serviceName: release-name-server
  selector:
    matchLabels:
      app.kubernetes.io/name: server
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: server
  template:
    metadata:
      annotations:
        checksum/config: a1ad9b5ea7abf59349050a88990ab86567056804278f3796f1bac16c18ae2d23
        checksum/config2: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/config3: 93a20a49f4d8181589c5aa689db1a126d284cc14b8ba8d203078c5b3c6dd2056
        checksum/configTornjak: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      labels:
        app.kubernetes.io/name: server
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: server
    spec:
      serviceAccountName: release-name-server
      shareProcessNamespace: true
      securityContext:
        {}
      containers:
        - name: spire-server
          securityContext:
            {}
          image: ghcr.io/spiffe/spire-server:1.7.2
          imagePullPolicy: IfNotPresent
          args:
            - -expandEnv
            - -config
            - /run/spire/config/server.conf
          env:
          - name: PATH
            value: "/opt/spire/bin:/bin"
          ports:
            - name: grpc
              containerPort: 8081
              protocol: TCP
            - containerPort: 8080
              name: healthz
          livenessProbe:
            httpGet:
              path: /live
              port: healthz
            failureThreshold: 2
            initialDelaySeconds: 15
            periodSeconds: 60
            timeoutSeconds: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: healthz
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            {}
          volumeMounts:
            - name: spire-server-socket
              mountPath: /tmp/spire-server/private
              readOnly: false
            - name: spire-config
              mountPath: /run/spire/config
              readOnly: true
            - name: spire-data
              mountPath: /run/spire/data
              readOnly: false
            
        - name: spire-controller-manager
          securityContext:
            {}
          image: ghcr.io/spiffe/spire-controller-manager:0.2.3
          imagePullPolicy: IfNotPresent
          args:
            - --config=controller-manager-config.yaml
          ports:
            - name: https
              containerPort: 9443
              protocol: TCP
            - containerPort: 8083
              name: healthz
          livenessProbe:
            httpGet:
              path: /healthz
              port: healthz
          readinessProbe:
            httpGet:
              path: /readyz
              port: healthz
          resources:
            {}
          volumeMounts:
            - name: spire-server-socket
              mountPath: /tmp/spire-server/private
              readOnly: true
            - name: controller-manager-config
              mountPath: /controller-manager-config.yaml
              subPath: controller-manager-config.yaml
              readOnly: true
            - name: spire-controller-manager-tmp
              mountPath: /tmp
              readOnly: false
      volumes:
        - name: spire-config
          configMap:
            name: release-name-server
        - name: spire-server-socket
          emptyDir: {}
        - name: spire-controller-manager-tmp
          emptyDir: {}
        - name: controller-manager-config
          configMap:
            name: release-name-controller-manager
  volumeClaimTemplates:
    - metadata:
        name: spire-data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
---
# Source: spire/charts/spiffe-csi-driver/templates/spiffe-csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: "csi.spiffe.io"
spec:
  # Only ephemeral, inline volumes are supported. There is no need for a
  # controller to provision and attach volumes.
  attachRequired: false

  # Request the pod information which the CSI driver uses to verify that an
  # ephemeral mount was requested.
  podInfoOnMount: true

  # Don't change ownership on the contents of the mount since the Workload API
  # Unix Domain Socket is typically open to all (i.e. 0777).
  fsGroupPolicy: None

  # Declare support for ephemeral volumes only.
  volumeLifecycleModes:
    - Ephemeral
---
# Source: spire/charts/spire-server/templates/controller-manager-cluster-ids.yaml
apiVersion: spire.spiffe.io/v1alpha1
kind: ClusterSPIFFEID
metadata:
  name: release-name-controller-manager-service-account-based
  namespace: default
spec:
  spiffeIDTemplate: "spiffe://{{ .TrustDomain }}/ns/{{ .PodMeta.Namespace }}/sa/{{ .PodSpec.ServiceAccountName }}"
---
# Source: spire/charts/spire-server/templates/controller-manager-webhook.yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: release-name-controller-manager-webhook
webhooks:
  - admissionReviewVersions: ["v1"]
    clientConfig:
      service:
        name: release-name-controller-manager-webhook
        namespace: default
        path: /validate-spire-spiffe-io-v1alpha1-clusterfederatedtrustdomain
    failurePolicy: Ignore # Actual value to be set by post install/upgrade hooks
    name: vclusterfederatedtrustdomain.kb.io
    rules:
      - apiGroups: ["spire.spiffe.io"]
        apiVersions: ["v1alpha1"]
        operations: ["CREATE", "UPDATE"]
        resources: ["clusterfederatedtrustdomains"]
    sideEffects: None
  - admissionReviewVersions: ["v1"]
    clientConfig:
      service:
        name: release-name-controller-manager-webhook
        namespace: default
        path: /validate-spire-spiffe-io-v1alpha1-clusterspiffeid
    failurePolicy: Ignore # Actual value to be set by post install/upgrade hooks
    name: vclusterspiffeid.kb.io
    rules:
      - apiGroups: ["spire.spiffe.io"]
        apiVersions: ["v1alpha1"]
        operations: ["CREATE", "UPDATE"]
        resources: ["clusterspiffeids"]
    sideEffects: None
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-server-post-install
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-server-post-upgrade
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-server-pre-upgrade
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-server-post-install
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
rules:
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    resourceNames: ["release-name-controller-manager-webhook"]
    verbs: ["get", "patch"]
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-server-post-upgrade
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
rules:
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    resourceNames: ["release-name-controller-manager-webhook"]
    verbs: ["get", "patch"]
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-server-pre-upgrade
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
rules:
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations"]
    resourceNames: ["release-name-controller-manager-webhook"]
    verbs: ["get", "patch"]
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-server-post-install
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
subjects:
  - kind: ServiceAccount
    name: release-name-server-post-install
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-server-post-install
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-server-post-upgrade
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
subjects:
  - kind: ServiceAccount
    name: release-name-server-post-upgrade
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-server-post-upgrade
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-server-pre-upgrade
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
subjects:
  - kind: ServiceAccount
    name: release-name-server-pre-upgrade
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-server-pre-upgrade
  apiGroup: rbac.authorization.k8s.io
---
# Source: spire/charts/spire-server/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-server-test-connection"
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: test
  annotations:
    "helm.sh/hook": test
spec:
  securityContext:
    {}
  containers:
    - name: curl
      image: cgr.dev/chainguard/bash:latest@sha256:96ab1600d945b4a99c8610b5c8b31e346da63dc20573a26bb0777dd0190db5d4
      command: ['bash']
      args:
        - -c
        - |
          curl -f -s 'https://release-name-server:8081'
          NOCA=$?
          curl -k -f -s 'https://release-name-server:8081'
          IGNORECA=$?
          echo $NOCA $IGNORECA
          if [ $NOCA -eq 60 -a $IGNORECA -eq 22 ]; then
            # We were able to connect to the server but didn't recognize the ca (60) and the page not found (22) because we're not using grpc
            exit 0
          fi
          exit 1
      securityContext:
        {}
  restartPolicy: Never
---
# Source: spire/charts/spire-server/templates/post-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-server-post-install
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
spec:
  template:
    metadata:
      name: release-name-server-post-install
    spec:
      restartPolicy: Never
      serviceAccountName: release-name-server-post-install
      securityContext:
        {}
      containers:
      - name: post-install-job
        securityContext:
          {}
        image: docker.io/rancher/kubectl:v1.26.0
        args:
          - patch
          - validatingwebhookconfiguration
          - release-name-controller-manager-webhook
          - --type=strategic
          - -p
          - |
            {
              "webhooks":[
                {
                  "name":"vclusterspiffeid.kb.io",
                  "failurePolicy":"Fail"
                },
                {
                  "name":"vclusterfederatedtrustdomain.kb.io",
                  "failurePolicy":"Fail"
                }
              ]
            }
---
# Source: spire/charts/spire-server/templates/post-upgrade-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-server-post-upgrade
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
spec:
  template:
    metadata:
      name: release-name-server-post-upgrade
    spec:
      restartPolicy: Never
      serviceAccountName: release-name-server-post-upgrade
      securityContext:
        {}
      containers:
      - name: post-upgrade-job
        securityContext:
          {}
        image: docker.io/rancher/kubectl:v1.26.0
        args:
          - patch
          - validatingwebhookconfiguration
          - release-name-controller-manager-webhook
          - --type=strategic
          - -p
          - |
            {
              "webhooks":[
                {
                  "name":"vclusterspiffeid.kb.io",
                  "failurePolicy":"Fail"
                },
                {
                  "name":"vclusterfederatedtrustdomain.kb.io",
                  "failurePolicy":"Fail"
                }
              ]
            }
---
# Source: spire/charts/spire-server/templates/pre-upgrade-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-server-pre-upgrade
  namespace: default
  labels:
    helm.sh/chart: spire-server-0.1.0
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.7.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation, hook-succeeded, hook-failed
spec:
  template:
    metadata:
      name: release-name-server-pre-upgrade
    spec:
      restartPolicy: Never
      serviceAccountName: release-name-server-pre-upgrade
      securityContext:
        {}
      containers:
      - name: post-install-job
        securityContext:
          {}
        image: docker.io/rancher/kubectl:v1.26.0
        args:
          - patch
          - validatingwebhookconfiguration
          - release-name-controller-manager-webhook
          - --type=strategic
          - -p
          - |
            {
              "webhooks":[
                {
                  "name":"vclusterspiffeid.kb.io",
                  "failurePolicy":"Ignore"
                },
                {
                  "name":"vclusterfederatedtrustdomain.kb.io",
                  "failurePolicy":"Ignore"
                }
              ]
            }
